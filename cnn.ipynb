{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2 as cv\n",
    "import matplotlib.pyplot as plt\n",
    "import skimage.feature\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# blurred_img = cv2.GaussianBlur(labeled_img, (5,5), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_input_img(name):\n",
    "    labeled_img = cv.imread('D:/ML work/NOAA Sea Lion count/Data/Semantics Segmentation/' + name + '-processed.tif')\n",
    "    original_img = cv.imread('D:/ML work/NOAA Sea Lion count/Data/Train/' + name + '.jpg')\n",
    "    \n",
    "    return original_img, labeled_img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cnn_model_upsampled(features, labels, mode):\n",
    "    \n",
    "   \n",
    "\n",
    "    \"\"\"Model function for CNN.\"\"\"\n",
    "\n",
    "    # Convolutional Layer #1\n",
    "    conv1 = tf.layers.conv2d(\n",
    "      inputs=features,\n",
    "      filters=15,\n",
    "      kernel_size=[5, 5],\n",
    "      padding=\"same\",\n",
    "      activation=tf.nn.relu)\n",
    "    \n",
    "    conv2 = tf.layers.conv2d(\n",
    "        inputs = conv1,\n",
    "        filters = 25,\n",
    "        kernel_size = [5, 5],\n",
    "        padding = \"same\",\n",
    "        activation = tf.nn.relu)\n",
    "    \n",
    "    conv3 = tf.layers.conv2d(\n",
    "        inputs = conv2,\n",
    "        filters = 50,\n",
    "        kernel_size = [5, 5],\n",
    "        padding = \"same\",\n",
    "        activation = tf.nn.relu)\n",
    "\n",
    "    # Pooling Layer #1\n",
    "#     pool1 = tf.layers.max_pooling2d(inputs = conv1, pool_size=[2, 2], strides=2)\n",
    "\n",
    "#     # Convolutional Layer #2 and Pooling Layer #2\n",
    "#     conv4 = tf.layers.conv2d(\n",
    "#       inputs=pool1,\n",
    "#       filters=64,\n",
    "#       kernel_size=[5, 5],\n",
    "#       padding=\"same\",\n",
    "#       activation=tf.nn.relu)\n",
    "    \n",
    "#     conv5 = tf.layers.conv2d(\n",
    "#       inputs=conv4,\n",
    "#       filters=70,\n",
    "#       kernel_size=[5, 5],\n",
    "#       padding=\"same\",\n",
    "#       activation=tf.nn.relu)\n",
    "    \n",
    "#     conv6 = tf.layers.conv2d(\n",
    "#       inputs=conv5,\n",
    "#       filters=85,\n",
    "#       kernel_size=[5, 5],\n",
    "#       padding=\"same\",\n",
    "#       activation=tf.nn.relu)\n",
    "    \n",
    "#     pool2 = tf.layers.max_pooling2d(inputs = conv6, pool_size=[2, 2], strides=2)\n",
    "    \n",
    "#     conv7 = tf.layers.conv2d(\n",
    "#         inputs=conv1,\n",
    "#         filters=5,\n",
    "#         kernel_size=[5, 5],\n",
    "#         padding=\"same\",\n",
    "#         activation=tf.nn.relu)\n",
    "    \n",
    "#     deconv7 = tf.layers.conv2d_transpose(\n",
    "#         inputs = conv7,\n",
    "#         filters = 5,\n",
    "#         kernel_size = [5, 5],\n",
    "#         padding = \"same\",\n",
    "#         activation = tf.nn.relu)\n",
    "    \n",
    "#     unpool2 = tf.gradients(pool2, conv6)[0]\n",
    "    \n",
    "#     deconv6 = tf.layers.conv2d_transpose(\n",
    "#         inputs = unpool2,\n",
    "#         filters = 70,\n",
    "#         kernel_size = [5, 5],\n",
    "#         padding = \"same\",\n",
    "#         activation = tf.nn.relu)\n",
    "    \n",
    "#     deconv5 = tf.layers.conv2d_transpose(\n",
    "#         inputs = deconv6,\n",
    "#         filters = 70,\n",
    "#         kernel_size = [5, 5],\n",
    "#         padding = \"same\",\n",
    "#         activation = tf.nn.relu)\n",
    "    \n",
    "#     deconv4 = tf.layers.conv2d_transpose(\n",
    "#         inputs = deconv5,\n",
    "#         filters = 50,\n",
    "#         kernel_size = [5, 5],\n",
    "#         padding = \"same\",\n",
    "#         activation = tf.nn.relu)\n",
    "    \n",
    "#     ps1 = tf.gradients(deconv7, conv3)[0]\n",
    "#     unpool1 = \n",
    "    \n",
    "    deconv3 = tf.layers.conv2d_transpose(\n",
    "        inputs = conv3,\n",
    "        filters = 25,\n",
    "        kernel_size = [5, 5],\n",
    "        padding = \"same\",\n",
    "        activation = tf.nn.relu)\n",
    "    \n",
    "    deconv2 = tf.layers.conv2d_transpose(\n",
    "        inputs = deconv3,\n",
    "        filters = 15,\n",
    "        kernel_size = [5, 5],\n",
    "        padding = \"same\",\n",
    "        activation = tf.nn.relu)\n",
    "    \n",
    "    deconv1 = tf.layers.conv2d_transpose(\n",
    "        inputs = deconv2,\n",
    "        filters = 3,\n",
    "        kernel_size = [5, 5],\n",
    "        padding = \"same\",\n",
    "        activation = tf.nn.relu)\n",
    "    \n",
    "    return deconv1\n",
    "\n",
    "#     # Dense Layer\n",
    "#     pool2_flat = tf.reshape(pool2, [-1, pool2.shape[1] * pool2.shape[0] * pool2.shape[2]])\n",
    "#     dense = tf.layers.dense(inputs=pool2_flat, units=1024, activation=tf.nn.relu)\n",
    "# #     dropout = tf.layers.dropout(\n",
    "# #       inputs=dense, rate=0.4, training=mode == tf.estimator.ModeKeys.TRAIN)\n",
    "\n",
    "#     # Logits Layer\n",
    "#     logits = tf.layers.dense(inputs=dense, units=5)\n",
    "\n",
    "#     predictions = {\n",
    "#       # Generate predictions (for PREDICT and EVAL mode)\n",
    "#       \"classes\": tf.argmax(input=logits, axis=1),\n",
    "#       # Add `softmax_tensor` to the graph. It is used for PREDICT and by the\n",
    "#       # `logging_hook`.\n",
    "#       \"probabilities\": tf.nn.softmax(logits, name=\"softmax_tensor\")\n",
    "#     }\n",
    "\n",
    "#     if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "#     return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)\n",
    "\n",
    "#     # Calculate Loss (for both TRAIN and EVAL modes)\n",
    "#     onehot_labels = tf.one_hot(indices=tf.cast(labels, tf.int32), depth=10)\n",
    "#     loss = tf.losses.softmax_cross_entropy(\n",
    "#       onehot_labels=onehot_labels, logits=logits)\n",
    "\n",
    "#     # Configure the Training Op (for TRAIN mode)\n",
    "#     if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "#     optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)\n",
    "#     train_op = optimizer.minimize(\n",
    "#         loss=loss,\n",
    "#         global_step=tf.train.get_global_step())\n",
    "#     return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)\n",
    "\n",
    "#     # Add evaluation metrics (for EVAL mode)\n",
    "#     eval_metric_ops = {\n",
    "#       \"accuracy\": tf.metrics.accuracy(\n",
    "#           labels=labels, predictions=predictions[\"classes\"])}\n",
    "#     return tf.estimator.EstimatorSpec(\n",
    "#       mode=mode, loss=loss, eval_metric_ops=eval_metric_ops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "features, labels = get_input_img('0')\n",
    "\n",
    "height = 4000\n",
    "width = 6000\n",
    "channel = 3\n",
    "batch_length_vertical = 20\n",
    "batch_length_horizontal = 30\n",
    "mini_batch = 100\n",
    "\n",
    "features = np.pad(features, ((4000 - features.shape[0], 0), (6000 - features.shape[1], 0), (0,0)), mode = 'constant')\n",
    "labels = np.pad(labels, ((4000 - labels.shape[0], 0), (6000 - labels.shape[1], 0), (0,0)), mode = 'constant')\n",
    "\n",
    "features = np.reshape(features, [-1, int(height/batch_length_vertical), int(width/batch_length_horizontal), channel])\n",
    "labels = np.reshape(labels, [-1, int(height/batch_length_vertical), int(width/batch_length_horizontal), channel])\n",
    "\n",
    "features = np.reshape(features, [int(features.shape[0]/mini_batch), mini_batch, features.shape[1], features.shape[2], channel])\n",
    "labels = np.reshape(labels, [int(labels.shape[0]/mini_batch), mini_batch, labels.shape[1], labels.shape[2], channel])\n",
    "\n",
    "# tf_features = tf.cast(features, tf.float32)\n",
    "# tf_labels = tf.cast(labels, tf.float32)\n",
    "\n",
    "# # Input Layer\n",
    "# tf_features = tf.reshape(tf_features, [-1, height,  width,  channel])\n",
    "# tf_labels = tf.reshape(tf_labels, [-1, height, width, channel])\n",
    "\n",
    "tf_features = tf.placeholder(tf.float16, [100, int(height/batch_length_vertical), int(width/batch_length_horizontal), channel], name = 'features')\n",
    "tf_labels = tf.placeholder(tf.float16, [100, int(height/batch_length_vertical), int(width/batch_length_horizontal), channel], 'labels')\n",
    "\n",
    "output = cnn_model_upsampled(tf_features, tf_labels, tf.estimator.ModeKeys.TRAIN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(100), Dimension(200), Dimension(200), Dimension(3)])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((6, 100, 200, 200, 3), (6, 100, 200, 200, 3))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.shape, labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.reshape(tf_features, [-1, int(height/batch_length), int(width/batch_length), channel])[399].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# np.reshape(features, [-1, int(height/batch_length), int(width/batch_length), channel]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00/100] trainLoss: 20.3281 trainAcc: 0.57\n",
      "[01/100] trainLoss: 13.0469 trainAcc: 0.60\n",
      "[02/100] trainLoss: 13.0469 trainAcc: 0.60\n",
      "[03/100] trainLoss: 13.0703 trainAcc: 0.60\n",
      "[04/100] trainLoss: 13.0469 trainAcc: 0.60\n",
      "[05/100] trainLoss: 13.0469 trainAcc: 0.60\n",
      "[06/100] trainLoss: 13.0234 trainAcc: 0.60\n",
      "[07/100] trainLoss: 13.0547 trainAcc: 0.60\n",
      "[08/100] trainLoss: 13.0469 trainAcc: 0.60\n",
      "[09/100] trainLoss: 13.0781 trainAcc: 0.60\n",
      "[10/100] trainLoss: 13.0625 trainAcc: 0.60\n",
      "[11/100] trainLoss: 13.0547 trainAcc: 0.60\n",
      "[12/100] trainLoss: 13.0938 trainAcc: 0.60\n",
      "[13/100] trainLoss: 13.0391 trainAcc: 0.60\n",
      "[14/100] trainLoss: 13.0703 trainAcc: 0.60\n",
      "[15/100] trainLoss: 13.0625 trainAcc: 0.60\n",
      "[16/100] trainLoss: 13.0625 trainAcc: 0.60\n",
      "[17/100] trainLoss: 13.0625 trainAcc: 0.60\n",
      "[18/100] trainLoss: 13.0547 trainAcc: 0.60\n",
      "[19/100] trainLoss: 13.0469 trainAcc: 0.60\n",
      "[20/100] trainLoss: 13.0625 trainAcc: 0.60\n",
      "[21/100] trainLoss: 13.0547 trainAcc: 0.60\n",
      "[22/100] trainLoss: 13.0703 trainAcc: 0.60\n",
      "[23/100] trainLoss: 13.0625 trainAcc: 0.60\n",
      "[24/100] trainLoss: 13.0547 trainAcc: 0.60\n",
      "[25/100] trainLoss: 13.0469 trainAcc: 0.60\n",
      "[26/100] trainLoss: 13.0469 trainAcc: 0.60\n",
      "[27/100] trainLoss: 13.0703 trainAcc: 0.60\n",
      "[28/100] trainLoss: 13.0625 trainAcc: 0.60\n",
      "[29/100] trainLoss: 13.0625 trainAcc: 0.60\n",
      "[30/100] trainLoss: 13.0625 trainAcc: 0.60\n",
      "[31/100] trainLoss: 13.0625 trainAcc: 0.60\n",
      "[32/100] trainLoss: 13.0703 trainAcc: 0.60\n",
      "[33/100] trainLoss: 13.0625 trainAcc: 0.60\n",
      "[34/100] trainLoss: 13.0469 trainAcc: 0.60\n",
      "[35/100] trainLoss: 13.0625 trainAcc: 0.60\n",
      "[36/100] trainLoss: 13.0469 trainAcc: 0.60\n",
      "[37/100] trainLoss: 13.0703 trainAcc: 0.60\n",
      "[38/100] trainLoss: 13.0469 trainAcc: 0.60\n",
      "[39/100] trainLoss: 13.0703 trainAcc: 0.60\n",
      "[40/100] trainLoss: 13.0625 trainAcc: 0.60\n",
      "[41/100] trainLoss: 13.0625 trainAcc: 0.60\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-5c9813ea3c1a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     33\u001b[0m                 \u001b[0mfeed_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mtf_features\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbatch_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtf_labels\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbatch_labels\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m                 \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msess_cost\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msess_accuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcost\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccr\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m     \u001b[1;31m#             train_loss.append(sess.run(cost, feed_dict = {features_pl: epoch_x, labels_pl: epoch_y}))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[1;31m#             train_accuracy.append(sess.run(accr, feed_dict = {features_pl: epoch_x, labels_pl: epoch_y}))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\AnacondaIDE\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    893\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 895\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    896\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\AnacondaIDE\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1122\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1124\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1125\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\AnacondaIDE\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1319\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[1;32m-> 1321\u001b[1;33m                            options, run_metadata)\n\u001b[0m\u001b[0;32m   1322\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1323\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\AnacondaIDE\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1325\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1326\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1327\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1328\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\AnacondaIDE\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1304\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[0;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1306\u001b[1;33m                                    status, run_metadata)\n\u001b[0m\u001b[0;32m   1307\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1308\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# y = tf.cast(labels, tf.float32) #tf.placeholer(tf.float32, [None, n_output])\n",
    "# y = tf.reshape(labels, [-1, labels.shape[0], labels.shape[1], labels.shape[2]])\n",
    "\n",
    "# y = labels\n",
    "epochs = 100\n",
    "\n",
    "\n",
    "# with tf.device(\"/gpu:0\"):\n",
    "\n",
    "for i in range(labels.shape[0]):\n",
    "    batch_features, batch_labels = features[i], labels[i]\n",
    "\n",
    "    #cost\n",
    "    cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=output, labels=batch_labels)\n",
    "    cost = tf.reduce_mean( cross_entropy )\n",
    "\n",
    "    #optimizer\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=0.0001).minimize(cost)\n",
    "\n",
    "    # Accuracy\n",
    "    corr = tf.equal(tf.argmax(tf_labels,1), tf.argmax(output, 1))\n",
    "    accr = tf.reduce_mean(tf.cast(corr, \"float\"))\n",
    "\n",
    "    with tf.Session(config=tf.ConfigProto(log_device_placement=True)) as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            epoch_loss = 0\n",
    "            train_loss = []; train_accuracy = []\n",
    "\n",
    "            for i in range(features.shape[0]):\n",
    "\n",
    "                feed_dict = {tf_features: batch_features, tf_labels: batch_labels}\n",
    "\n",
    "                _, sess_cost, sess_accuracy = sess.run([optimizer, cost, accr], feed_dict = feed_dict)\n",
    "    #             train_loss.append(sess.run(cost, feed_dict = {features_pl: epoch_x, labels_pl: epoch_y}))\n",
    "    #             train_accuracy.append(sess.run(accr, feed_dict = {features_pl: epoch_x, labels_pl: epoch_y}))\n",
    "                train_loss.append(sess_cost)\n",
    "                train_accuracy.append(sess_accuracy)\n",
    "\n",
    "    #             epoch_loss += c\n",
    "\n",
    "    #             print('Epoch', epoch, 'completed out of', epochs, 'loss:', epoch_loss)\n",
    "\n",
    "            # Average loss and accuracy\n",
    "            train_loss = np.mean(train_loss)\n",
    "            train_accuracy = np.mean(train_accuracy)\n",
    "\n",
    "            print (\"[%02d/%02d] trainLoss: %.4f trainAcc: %.2f\" \n",
    "                   % (epoch, epochs, train_loss, train_accuracy))\n",
    "\n",
    "    #         correct = tf.equal(tf.argmax(output, 1), tf.argmax(labels, 1))\n",
    "\n",
    "    #         accuracy = tf.reduce_mean(tf.cast(correct, 'float'))\n",
    "    #         print('Accuracy:',accuracy.eval({x:features, y:labels}))\n",
    "\n",
    "    # if __name__ == \"__main__\":\n",
    "    # tf.app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(1), Dimension(4000), Dimension(6000), Dimension(3)])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'conv2d_transpose_13/Relu:0' shape=(1, 4000, 6000, 3) dtype=float32>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(1), Dimension(4000), Dimension(6000), Dimension(3)])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(1), Dimension(4000), Dimension(6000), Dimension(3)])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3744, 5616, 3)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(1), Dimension(4000), Dimension(6000), Dimension(3)])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
